import math
import numpy as np
from scipy.spatial import distance
import sklearn.model_selection as sklms
from sklearn.cluster import KMeans

def train_test_split(k, X, test_size=.15, alt_test_size=.45, random_state=73):
    '''
    Function that generates a train an test set for a clustering model. 
    If the test set generated by default does not have at least n samples 
    as the number of centroids, it tries to sample the data somehow 
    that the number of the test sample to be bigger.
    
    Parameters
    ----------
    k : int
        Denotes the number of centroids
    X : pandas.DataFrame
        The original collection of samples
    
    Returns
    -------
    X_train, X_test : pandas.DataFrame
    '''
    X_train, X_test = sklms.train_test_split(X, test_size=test_size, random_state=random_state)
    if (len(X_test) < k):
        X_train, X_test = sklms.train_test_split(X, test_size=alt_test_size, random_state=random_state)
    return X_train, X_test

def closest_centroid(x, centroids):
    '''
    Function for retrieving the closest centroid to the given observation 
    in terms of the Euclidean distance.
    
    Parameters
    ----------
    x : array
        An array containing the observation to be matched to the nearest centroid
    centroids : array
        An array containing the centroids
    
    Returns
    -------
    min_centroid : array
        The centroid closes to the obs 
    '''
    min_distance = math.inf
    min_centroid = 0
    
    for c in centroids:
        dist = distance.euclidean(x, c)
        if dist < min_distance:
            min_distance = dist
            min_centroid = c
            
    return min_centroid

def classic_prediction_strength(k, C_train, C_valid):
    '''
    Function for calculating the prediction strength 
    of clustering for a given number of clusters
    
    Parameters
    ----------
    k : int
        The number of clusters
    C_train : array
        Labels predicted from the trainning model
    C_valid : array
        Labels from the validation
        
    Returns
    -------
    prediction_strength : float
        Calculated prediction strength
    '''
    n_valid = len(C_valid)
    D = np.zeros(shape=(n_valid, n_valid))
    
    for l1, c1 in zip(C_train, list(range(n_valid))):
        for l2, c2 in zip(C_valid, list(range(n_valid))):
            if c1 != c2 and l1 == l2:
                D[c1,c2] = 1.0
    
    # calculate the prediction strengths for each cluster
    ss = []
    for j in range(k):

        s = 0
        n_examples_j = np.count_nonzero(C_valid == j)

        if n_examples_j <= 1:
            ss.append(math.inf) # no points in the cluster
        else:
            for l1, c1 in zip(C_train, list(range(n_valid))):
                for l2, c2 in zip(C_valid, list(range(n_valid))):
                    # not diagonal & same cluster in both partitions 
                    # and classifyied in current k cluster
                    if c1 != c2 and l1 == l2 and l1 == j:
                        s += D[c1,c2]

            p = n_examples_j * (n_examples_j - 1)
            ss.append(s / p)
 
    prediction_strength = min(ss)

    return prediction_strength

def prediction_strength_of_clusters(X, K):

    X_train, X_valid = sklms.train_test_split(X, test_size=0.25, random_state=73)
    train_model = KMeans(n_clusters=K, random_state=73, n_init='auto').fit(X_train)
    valid_model = KMeans(n_clusters=K, random_state=73, n_init='auto').fit(X_valid)
    train_centroids = train_model.cluster_centers_

    if K == 1:
        return [(K, 1, train_centroids)]
    else:
        ps = prediction_strength(K, train_centroids, X_valid.to_numpy(), valid_model.labels_)
        return prediction_strength_of_clusters(X, K - 1) + [(K, ps, train_centroids)]

def prediction_strength(k, train_centroids, X_test, test_labels):
    '''
    Function for calculating the prediction strength of clustering for
    a given number of clusters
    
    Parameters
    ----------
    k : int
        The number of clusters
    train_centroids : array
        Centroids from the clustering on the training set
    X_test : array
        Test set observations
    test_labels : array
        Labels predicted for the test set
        
    Returns
    -------
    prediction_strength : float
        Calculated prediction strength
    '''
    n_test = len(X_test)
    
    # populate the co-membership matrix
    D = np.zeros(shape=(n_test, n_test))
    for x1, c1 in zip(X_test, list(range(n_test))):
        for x2, c2 in zip(X_test, list(range(n_test))):
            # checks not to be the same sample
            if c1 != c2:
                # when 2 samples are part of the same cluster in the matrix of samples they are assigned to 1.
                if tuple(closest_centroid(x1, train_centroids)) == tuple(closest_centroid(x2, train_centroids)):
                    D[c1, c2] = 1.0
    
    # calculate the prediction strengths for each cluster
    ss = []
    for j in range(k):

        s = 0
        examples_j = X_test[test_labels == j, :].tolist()
        n_examples_j = len(examples_j)

        if n_examples_j <= 1:
            ss.append(math.inf) # no points in the cluster
        else:
            for x1, l1, c1 in zip(X_test, test_labels, list(range(n_test))):
                for x2, l2, c2 in zip(X_test, test_labels, list(range(n_test))):
                    # checks if 2 differents samples were marked as part of the
                    # same cluster. If they were, consults the co-membership matrix
                    # to add 1 point if they were part of the same cluster thanks to the
                    # euclidian vector, otherwise the sum do not modify the cluster prediction strengths.
                    if c1 != c2 and l1 == l2 and l1 == j:
                        s += D[c1,c2]

            p = n_examples_j * (n_examples_j - 1)
            ss.append(s / p)
 
    prediction_strength = min(ss)

    return prediction_strength

